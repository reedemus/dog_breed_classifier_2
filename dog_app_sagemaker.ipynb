{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zkRhjEX2a0uK"
   },
   "source": [
    "# Train and Deploy Custom Model in AWS\n",
    "\n",
    "## Project: Train, Evaluate and Deploy Dog Identification App in SageMaker\n",
    "---\n",
    "### Why We're Here \n",
    "In this notebook, we will train and deploy a **custom model** in SageMaker. Specifically, the pretrained PyTorch model from  [Dog Breed Classifier](https://github.com/reedemus/dog_breed_classifier) project will be used as an example for this exercise. \n",
    "### The Road Ahead\n",
    "\n",
    "We break the notebook into separate steps. Feel free to use the links below to navigate the notebook.\n",
    "\n",
    "* [Step 0](#Step0): Install required packages\n",
    "* [Step 1](#step1): Upload the dataset into an S3 bucket\n",
    "* [Step 2](#step2): Create the custom model\n",
    "* [Step 3](#step3): Completing a training script\n",
    "* [Step 4](#step4): Training and deploying the custom model\n",
    "* [Step 5](#step5): Evaluating the performance\n",
    "---\n",
    "<a id='step0'></a>\n",
    "## Step 0: Install required packages\n",
    "Install missing packages and dependencies in the instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r code/requirements.txt (line 1)) (4.61.1)\n",
      "Collecting torch>=1.10.0\n",
      "  Downloading torch-1.10.2-cp36-cp36m-manylinux1_x86_64.whl (881.9 MB)\n",
      "     |████████████████████████████████| 881.9 MB 3.3 kB/s              |██████████████████████████████▎ | 836.0 MB 79.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torchvision>=0.11.0\n",
      "  Downloading torchvision-0.11.2-cp36-cp36m-manylinux1_x86_64.whl (23.3 MB)\n",
      "     |████████████████████████████████| 23.3 MB 30.3 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torch>=1.10.0->-r code/requirements.txt (line 2)) (0.8)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torch>=1.10.0->-r code/requirements.txt (line 2)) (3.10.0.0)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision>=0.11.0->-r code/requirements.txt (line 3)) (1.19.5)\n",
      "Collecting torch>=1.10.0\n",
      "  Downloading torch-1.10.1-cp36-cp36m-manylinux1_x86_64.whl (881.9 MB)\n",
      "     |████████████████████████████████| 881.9 MB 3.8 kB/s             ��█████████████████████         | 634.0 MB 90.1 MB/s eta 0:00:03.2 MB 364 kB/s eta 0:08:05 \n",
      "\u001b[?25hRequirement already satisfied: pillow!=8.3.0,>=5.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision>=0.11.0->-r code/requirements.txt (line 3)) (8.4.0)\n",
      "Installing collected packages: torch, torchvision\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.4.0\n",
      "    Uninstalling torch-1.4.0:\n",
      "      Successfully uninstalled torch-1.4.0\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.5.0\n",
      "    Uninstalling torchvision-0.5.0:\n",
      "      Successfully uninstalled torchvision-0.5.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fastai 1.0.61 requires nvidia-ml-py3, which is not installed.\u001b[0m\n",
      "Successfully installed torch-1.10.1 torchvision-0.11.2\n"
     ]
    }
   ],
   "source": [
    "!pip install -r code/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step1'></a>\n",
    "## Step 1: Upload the dataset to S3\n",
    "\n",
    "We will import the AWS SageMaker libraries and define helper functions for handling the dataset. We will download the dog dataset from [this URL](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/dogImages.zip) and extract it before uploading them into the bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xZjdgyqLa0uU"
   },
   "outputs": [],
   "source": [
    "# import the required libraries\n",
    "import requests\n",
    "import boto3\n",
    "import sagemaker\n",
    "from zipfile import ZipFile\n",
    "from tqdm.notebook import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define `downloadFile` and `extractFile` helper functions to download the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downloadFile(file_url, file_name, dir=None, chunk_size=1024):\n",
    "    '''Helper function to download file to specified directory\n",
    "\n",
    "    :param file_url: file download URL\n",
    "    :param file_name: file name to be saved.\n",
    "    :param dir: path where file is saved other than current directory (Default = current working directory)\n",
    "    :param chunk_size: size of file chunk to download (Default = 1024 bytes)\n",
    "    :returns: None\n",
    "    '''\n",
    "    saved_file_path = file_name\n",
    "    if dir != None and not os.path.exists(dir):\n",
    "        os.mkdir(dir)\n",
    "        saved_file_path = os.path.join(dir, file_name)\n",
    "\n",
    "    r = requests.get(file_url, stream=True)\n",
    "    total_size_in_bytes = len(r.content)\n",
    "    progress_bar = tqdm(total=total_size_in_bytes, unit='iB', unit_scale=True, desc=file_name)\n",
    "    \n",
    "    with open(saved_file_path, 'wb') as f:\n",
    "        for chunk in r.iter_content(chunk_size):\n",
    "            progress_bar.update(len(chunk))\n",
    "            # writing one chunk at a time to file\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "    progress_bar.close()\n",
    "    if total_size_in_bytes != 0 and progress_bar.n != total_size_in_bytes:\n",
    "       print(\"ERROR, something went wrong\")\n",
    "       return\n",
    "\n",
    "def extractFile(file_name):\n",
    "    '''Extracts compressed file in zip format into current directory\n",
    "    \n",
    "    :param file_name: file name\n",
    "    :returns: None\n",
    "    '''\n",
    "    # create a zipfile object and extract it to current directory\n",
    "    print(\"Extracting file...\")\n",
    "    with ZipFile(file_name, 'r') as z:\n",
    "        z.extractall()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the dataset into current directory. The default folder after extraction is `dogImages/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "M_l78X2Ma0uX"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9046f81e71d646e1ae84bb4f49c01e2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dogImages.zip:   0%|          | 0.00/1.13G [00:00<?, ?iB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting file...\n",
      "There are 8351 total dog images.\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import numpy as np\n",
    "\n",
    "dog_url = 'https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/dogImages.zip'\n",
    "\n",
    "downloadFile(dog_url, 'dogImages.zip')\n",
    "extractFile('dogImages.zip')\n",
    "\n",
    "# load filenames for human and dog images\n",
    "dog_files = np.array(glob(\"dogImages/*/*/*\"))\n",
    "\n",
    "# print number of images in each dataset\n",
    "print('There are %d total dog images.' % len(dog_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by creating a SageMaker session and specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data. This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these. Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the `sagemaker.get_execution_role()` with a the appropriate full IAM role arn string(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-ap-south-1-461678052840/dog-breed-classifier\n"
     ]
    }
   ],
   "source": [
    "# session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# create an S3 bucket\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "# Name of the dataset directory\n",
    "data_dir = 'dogImages'\n",
    "\n",
    "# set prefix, a descriptive name for a directory  \n",
    "prefix = 'dog-breed-classifier'\n",
    "\n",
    "# upload all data to S3\n",
    "input_dataset = sagemaker_session.upload_data(path=data_dir, bucket=bucket, key_prefix=prefix)\n",
    "print(input_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test cell\n",
    "Test that our data has been successfully uploaded. The cell below prints out the items in the S3 bucket and will throw an error if it is empty. We should see the contents of ```data_dir``` and perhaps some checkpoints. If there are any other files listed, then we may have some old model files that can be deleted via the S3 console (though, additional files shouldn't affect the performance of model developed in this notebook).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that data is in S3 bucket\n",
    "empty_check = []\n",
    "for obj in boto3.resource('s3').Bucket(bucket).objects.all():\n",
    "    empty_check.append(obj.key)\n",
    "    print(obj.key)\n",
    "\n",
    "assert len(empty_check) !=0, 'S3 bucket is empty.'\n",
    "print('Test passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CewCMDgDa0uY"
   },
   "source": [
    "---\n",
    "<a id='step2'></a>\n",
    "## Step 2: Create the custom model\n",
    "Create a CNN model to classify dog breed using transfer learning. The model is defined in `model.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "fwkiR2Tra0uY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorchvision\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmodels\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mmodels\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# Using feature extraction approach\u001b[39;49;00m\n",
      "\u001b[37m# =================================\u001b[39;49;00m\n",
      "\u001b[37m# Freeze the weights for all of the network except the final fully connected(FC) layer.\u001b[39;49;00m\n",
      "\u001b[37m# This last FC layer is replaced with a new one with random weights and only this layer is trained.\u001b[39;49;00m\n",
      "\u001b[37m# https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html#initialize-and-reshape-the-networks\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mDogBreedClassifier\u001b[39;49;00m:\n",
      "    \u001b[33m'''\u001b[39;49;00m\n",
      "\u001b[33m    Pretrained Resnet model with the output features in the last layer set to 133 nodes, which is the number of dog breed classes\u001b[39;49;00m\n",
      "\u001b[33m    '''\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\n",
      "        \u001b[34mpass\u001b[39;49;00m\n",
      "\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mget_model\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\n",
      "        \u001b[37m# ResNet 152-layer model\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.model_transfer = models.resnet152(pretrained=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "        \u001b[37m# Freeze the pre-trained weights,biases of all layers at first so it doesn't get updated during re-training\u001b[39;49;00m\n",
      "        \u001b[34mfor\u001b[39;49;00m param \u001b[35min\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.model_transfer.parameters():\n",
      "            param.requires_grad = \u001b[34mFalse\u001b[39;49;00m\n",
      "\n",
      "        \u001b[37m# Get the number of input features in the last FC layer\u001b[39;49;00m\n",
      "        \u001b[37m# Reinitialize output features to number of dog breed classes\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.input_features = \u001b[36mself\u001b[39;49;00m.model_transfer.fc.in_features\n",
      "        DOG_BREEDS_NUM = \u001b[34m133\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.model_transfer.fc = nn.Linear(\u001b[36mself\u001b[39;49;00m.input_features, DOG_BREEDS_NUM)\n",
      "\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mResNet-152 last fc layer:\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, models.resnet152().fc)\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mOur fc layer:\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mself\u001b[39;49;00m.model_transfer.fc)\n",
      "\n",
      "        \u001b[34mreturn\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.model_transfer\n"
     ]
    }
   ],
   "source": [
    "# Print the implementation using a Python syntax highlighter package\n",
    "!pygmentize 'code/model.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step3'></a>\n",
    "## Step 3: Create the training script\n",
    "Once the model is developed, we implement the training script ```train.py```. The script does the following steps:\n",
    "\n",
    "1. Loads training data from a specified directory\n",
    "2. Parses any training & model hyperparameters (ex. nodes in a neural network, training epochs, etc.)\n",
    "3. Instantiates a model of your design, with any specified hyperparams\n",
    "4. Trains that model\n",
    "5. Finally, saves the model so that it can be hosted/deployed later\n",
    "\n",
    "From the code below, notice a few things:\n",
    "\n",
    "- Model loading (`model_fn`) and saving code\n",
    "- Getting SageMaker's default hyperparameters\n",
    "- Loading the training data\n",
    "\n",
    "If you'd like to read more about model saving with __[torch.save](https://pytorch.org/tutorials/beginner/saving_loading_models.html)__, click on the provided links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36moptim\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36moptim\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DataLoader\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorchvision\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m datasets, transforms\n",
      "\n",
      "\u001b[37m# the following import is required for training to be robust to truncated images\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mPIL\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ImageFile\n",
      "ImageFile.LOAD_TRUNCATED_IMAGES = \u001b[34mTrue\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# imports the model in model.py by name\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mmodel\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DogBreedClassifier\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\n",
      "    \u001b[33m'''\u001b[39;49;00m\n",
      "\u001b[33m    Loads the PyTorch model from the `model_dir` directory.\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m    :param: model_dir = SageMaker's model directory\u001b[39;49;00m\n",
      "\u001b[33m    Reference:\u001b[39;49;00m\n",
      "\u001b[33m        https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html?highlight=model_fn#load-a-model\u001b[39;49;00m\n",
      "\u001b[33m    '''\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mLoading model...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[37m# Determine the device and construct the model\u001b[39;49;00m\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    model = DogBreedClassifier().get_model()\n",
      "\n",
      "    \u001b[37m# Load the stored model parameters\u001b[39;49;00m\n",
      "    model_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        model.load_state_dict(torch.load(f))\n",
      "\n",
      "    \u001b[37m# set to eval mode, could use no_grad\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m model.to(device).eval()\n",
      "\n",
      "\n",
      "\u001b[37m# model saving function\u001b[39;49;00m\n",
      "\u001b[37m# https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html?highlight=model_fn#save-the-model\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32msave_model\u001b[39;49;00m(model, model_dir):\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mSaving the model...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    model_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_path, \u001b[33m'\u001b[39;49;00m\u001b[33mwb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        \u001b[37m# save state dictionary\u001b[39;49;00m\n",
      "        torch.save(model.state_dict(), f)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_get_data_loader\u001b[39;49;00m(batch_size=\u001b[34m32\u001b[39;49;00m, training_dir=\u001b[33m'\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, validation_dir=\u001b[33m'\u001b[39;49;00m\u001b[33mvalid\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, testing_dir=\u001b[33m'\u001b[39;49;00m\u001b[33mtest\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m):\n",
      "    \u001b[33m'''\u001b[39;49;00m\n",
      "\u001b[33m    Create three separate data loaders for the training, validation, and test datasets\u001b[39;49;00m\n",
      "\u001b[33m    \u001b[39;49;00m\n",
      "\u001b[33m    :param: batch size = no of samples\u001b[39;49;00m\n",
      "\u001b[33m    :param: training_dir = folder path of training dataset\u001b[39;49;00m\n",
      "\u001b[33m    :param: validation_dir = folder path of validation dataset\u001b[39;49;00m\n",
      "\u001b[33m    :param: testing_dir = folder path of test dataset\u001b[39;49;00m\n",
      "\u001b[33m    :return: dictionary of the three dataloaders\u001b[39;49;00m\n",
      "\u001b[33m    '''\u001b[39;49;00m\n",
      "    \u001b[37m# Note: pretrained models from ImageNet requires input images to be shape (3 x h x w) with h,w >= 224 and \u001b[39;49;00m\n",
      "    \u001b[37m#       normalized to mean,std dev as per ImageNet (mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225])\u001b[39;49;00m\n",
      "    \u001b[37m#\u001b[39;49;00m\n",
      "    \u001b[37m# Reference: https://pytorch.org/vision/stable/models.html\u001b[39;49;00m\n",
      "    \n",
      "    \u001b[37m## Specify appropriate transforms, and batch_sizes\u001b[39;49;00m\n",
      "    train_dir = training_dir\n",
      "    valid_dir = validation_dir\n",
      "    test_dir = testing_dir\n",
      "    \u001b[37m# ResNet input image size\u001b[39;49;00m\n",
      "    IMG_SIZE = \u001b[34m224\u001b[39;49;00m\n",
      "    \u001b[37m# mean and std deviation of models trained on Imagenet dataset\u001b[39;49;00m\n",
      "    mean = [\u001b[34m0.485\u001b[39;49;00m, \u001b[34m0.456\u001b[39;49;00m, \u001b[34m0.406\u001b[39;49;00m]\n",
      "    std_dev = [\u001b[34m0.229\u001b[39;49;00m, \u001b[34m0.224\u001b[39;49;00m, \u001b[34m0.225\u001b[39;49;00m]\n",
      "\n",
      "    \u001b[37m# Data augmentation to create a variety of test images so the model learn to generalize better.\u001b[39;49;00m\n",
      "    \u001b[37m# Output is a tensor.\u001b[39;49;00m\n",
      "    preprocess_train = transforms.Compose([\n",
      "                                        transforms.RandomResizedCrop(IMG_SIZE),\n",
      "                                        transforms.RandomRotation(\u001b[34m20\u001b[39;49;00m),\n",
      "                                        transforms.RandomHorizontalFlip(),\n",
      "                                        transforms.ToTensor(),\n",
      "                                        transforms.Normalize( mean, std_dev)\n",
      "                                        ])\n",
      "\n",
      "    \u001b[37m# Data augmentation is not performed on validation and test datasets because the goal is not to create more data,\u001b[39;49;00m\n",
      "    \u001b[37m# but to resize and crop the images to the same size as the input image.\u001b[39;49;00m\n",
      "    \u001b[37m# Output is a tensor.\u001b[39;49;00m\n",
      "    preprocess_valid_test = transforms.Compose([\n",
      "                                        transforms.Resize(\u001b[34m256\u001b[39;49;00m),\n",
      "                                        transforms.CenterCrop(IMG_SIZE),\n",
      "                                        transforms.ToTensor(),\n",
      "                                        transforms.Normalize( mean, std_dev)\n",
      "                                        ])\n",
      "\n",
      "    train_dataset = datasets.ImageFolder(train_dir, transform=preprocess_train)\n",
      "    valid_dataset = datasets.ImageFolder(valid_dir, transform=preprocess_valid_test)\n",
      "    test_dataset = datasets.ImageFolder(test_dir, transform=preprocess_valid_test)\n",
      "    train_loader = DataLoader(train_dataset, batch_size, shuffle=\u001b[34mTrue\u001b[39;49;00m)\n",
      "    valid_loader = DataLoader(valid_dataset, batch_size, shuffle=\u001b[34mFalse\u001b[39;49;00m)\n",
      "    test_loader = DataLoader(test_dataset, batch_size, shuffle=\u001b[34mFalse\u001b[39;49;00m)\n",
      "\n",
      "    loaders_dict = { \u001b[33m'\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:train_loader, \u001b[33m'\u001b[39;49;00m\u001b[33mvalid\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:valid_loader, \u001b[33m'\u001b[39;49;00m\u001b[33mtest\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:test_loader }\n",
      "    \u001b[34mreturn\u001b[39;49;00m loaders_dict\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtrain\u001b[39;49;00m(model, loaders, n_epochs, optimizer, criterion, use_cuda):\n",
      "    \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    This is the training method that is called by the PyTorch training script. After\u001b[39;49;00m\n",
      "\u001b[33m    training, the model with the best validation accuracy is saved.\u001b[39;49;00m\n",
      "\u001b[33m    The parameters passed are as follows:\u001b[39;49;00m\n",
      "\u001b[33m    model        - The PyTorch model that we wish to train.\u001b[39;49;00m\n",
      "\u001b[33m    loaders      - The PyTorch DataLoader that should be used during training.\u001b[39;49;00m\n",
      "\u001b[33m    n_epochs     - The total number of epochs to train for.\u001b[39;49;00m\n",
      "\u001b[33m    criterion    - The loss function used for training. \u001b[39;49;00m\n",
      "\u001b[33m    optimizer    - The optimizer to use during training.\u001b[39;49;00m\n",
      "\u001b[33m    use_cuda     - uses gpu(True) or cpu(False).\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    \u001b[37m# initialize tracker for minimum validation loss\u001b[39;49;00m\n",
      "    valid_loss_min = np.Inf \n",
      "    \n",
      "    \u001b[34mfor\u001b[39;49;00m epoch \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(\u001b[34m1\u001b[39;49;00m, n_epochs+\u001b[34m1\u001b[39;49;00m):\n",
      "        \u001b[37m# initialize variables to monitor training and validation loss\u001b[39;49;00m\n",
      "        train_loss = \u001b[34m0.0\u001b[39;49;00m\n",
      "        valid_loss = \u001b[34m0.0\u001b[39;49;00m\n",
      "        \n",
      "        \u001b[37m###################\u001b[39;49;00m\n",
      "        \u001b[37m# train the model #\u001b[39;49;00m\n",
      "        \u001b[37m###################\u001b[39;49;00m\n",
      "        model.train()\n",
      "        \u001b[34mfor\u001b[39;49;00m batch_idx, (data, target) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(loaders[\u001b[33m'\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]):\n",
      "            \u001b[37m# move to GPU\u001b[39;49;00m\n",
      "            \u001b[34mif\u001b[39;49;00m use_cuda:\n",
      "                data, target = data.cuda(), target.cuda()\n",
      "            \n",
      "            \u001b[37m# zero the gradients accumulated from the previous backward propagation steps,\u001b[39;49;00m\n",
      "            \u001b[37m# make prediction, calculate the training loss, perform backpropagation, \u001b[39;49;00m\n",
      "            \u001b[37m# and finally update model weights and biases.\u001b[39;49;00m\n",
      "            optimizer.zero_grad()\n",
      "            output = model(data)\n",
      "            loss = criterion( output, target)\n",
      "            loss.backward()\n",
      "            optimizer.step()\n",
      "            \n",
      "            \u001b[37m# record the average training loss\u001b[39;49;00m\n",
      "            train_loss = train_loss + ((\u001b[34m1\u001b[39;49;00m / (batch_idx + \u001b[34m1\u001b[39;49;00m)) * (loss.data - train_loss))\n",
      "        \n",
      "        \u001b[37m######################    \u001b[39;49;00m\n",
      "        \u001b[37m# validate the model #\u001b[39;49;00m\n",
      "        \u001b[37m######################\u001b[39;49;00m\n",
      "        \u001b[37m# switch model to evalution mode and disable gradient calculations to reduce memory usage\u001b[39;49;00m\n",
      "        \u001b[37m# and speed up computations since no backpropagation is needed in evaluation.\u001b[39;49;00m\n",
      "        model.eval()\n",
      "        \u001b[34mwith\u001b[39;49;00m torch.no_grad():\n",
      "            \u001b[34mfor\u001b[39;49;00m batch_idx, (data, target) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(loaders[\u001b[33m'\u001b[39;49;00m\u001b[33mvalid\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]):\n",
      "                \u001b[37m# move to GPU\u001b[39;49;00m\n",
      "                \u001b[34mif\u001b[39;49;00m use_cuda:\n",
      "                    data, target = data.cuda(), target.cuda()\n",
      "\n",
      "                \u001b[37m# get prediction from our model, calculate the validation loss\u001b[39;49;00m\n",
      "                output = model(data)\n",
      "                loss = criterion( output, target)\n",
      "\n",
      "                \u001b[37m## update the average validation loss\u001b[39;49;00m\n",
      "                valid_loss = valid_loss + ((\u001b[34m1\u001b[39;49;00m / (batch_idx + \u001b[34m1\u001b[39;49;00m)) * (loss.data - valid_loss))\n",
      "\n",
      "            \u001b[37m# print training/validation statistics \u001b[39;49;00m\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mEpoch \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m: \u001b[39;49;00m\u001b[33m\\t\u001b[39;49;00m\u001b[33mTraining Loss: \u001b[39;49;00m\u001b[33m{:.3f}\u001b[39;49;00m\u001b[33m \u001b[39;49;00m\u001b[33m\\t\u001b[39;49;00m\u001b[33mValidation Loss: \u001b[39;49;00m\u001b[33m{:.3f}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(\n",
      "                epoch, train_loss, valid_loss))\n",
      "\n",
      "            \u001b[37m# save the model if validation loss has decreased\u001b[39;49;00m\n",
      "            \u001b[34mif\u001b[39;49;00m valid_loss < valid_loss_min:\n",
      "                valid_loss_min = valid_loss\n",
      "                save_model(model, args.model_dir)\n",
      "\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "    \u001b[37m# All of the model parameters and training parameters are sent as arguments\u001b[39;49;00m\n",
      "    \u001b[37m# when this script is executed, during a training job\u001b[39;49;00m\n",
      "    \n",
      "    \u001b[37m# Here we set up an argument parser to easily access the parameters\u001b[39;49;00m\n",
      "    parser = argparse.ArgumentParser()\n",
      "        \n",
      "    \u001b[37m# Training Parameters, given\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--batch-size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m32\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33minput batch size for training (default: 32)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m10\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mnumber of epochs to train (default: 10)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--lr\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.01\u001b[39;49;00m, metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mLR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "                        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mlearning rate (default: 0.01)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)    \n",
      "    \n",
      "    \u001b[37m# SageMaker parameters, like the directories for training data and saving models; set automatically\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--output-data-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_OUTPUT_DATA_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--valid-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_VALID\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--test-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TEST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    \n",
      "    \u001b[37m# args holds all passed-in arguments\u001b[39;49;00m\n",
      "    args = parser.parse_args()\n",
      "    \n",
      "    use_cuda = torch.cuda.is_available()\n",
      "    \u001b[34mif\u001b[39;49;00m use_cuda:\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mUsing GPU.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[34melse\u001b[39;49;00m:\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mUsing CPU.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[37m# Gets the train, validation and test data loaders\u001b[39;49;00m\n",
      "    data_loaders = _get_data_loader(args.batch_size, args.train_dir, args.valid_dir, args.test_dir)\n",
      "    \n",
      "    \u001b[37m# Build the model\u001b[39;49;00m\n",
      "    model = DogBreedClassifier().get_model()\n",
      "    \u001b[34mif\u001b[39;49;00m use_cuda:\n",
      "        model.cuda()\n",
      "\n",
      "    \u001b[37m# Define an optimizer and loss function for training\u001b[39;49;00m\n",
      "    criterion = nn.CrossEntropyLoss()\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
      "    \n",
      "    \u001b[37m# Trains and save the model\u001b[39;49;00m\n",
      "    train(model, data_loaders, args.epochs, optimizer, criterion, use_cuda)\n"
     ]
    }
   ],
   "source": [
    "# Print the implementation using a Python syntax highlighter package\n",
    "!pygmentize 'code/train.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kM8EPd9Ja0uk"
   },
   "source": [
    "---\n",
    "<a id='step4'></a>\n",
    "## Step 4: Create an Estimator\n",
    "When a custom model is constructed in SageMaker, an entry point must be specified. We need to provide a training script `train.py` which will be executed when the model is trained. To run the script, create a PyTorch `Estimator` and fill in the appropriate constructor arguments:\n",
    "\n",
    "- *entry_point*: The path to the Python script SageMaker runs for training and prediction.\n",
    "- *source_dir*: The path to the training script directory source_sklearn OR source_pytorch.\n",
    "- *role*: Role ARN, which was specified above.\n",
    "- *py_version*: version of Python.\n",
    "- *framework_version*: version of PyTorch.\n",
    "- *instance_count*: The number of training instances (should be left at 1).\n",
    "- *instance_type*: instantiate a new type of SageMaker instance for training.\n",
    ">Note: we could use the same instance that is running this notebook if desired\n",
    "- *sagemaker_session*: The session used to train on Sagemaker.\n",
    "- *hyperparameters (optional)*: A dictionary { 'name':value, ... } passed to the train function as hyperparameters.\n",
    "\n",
    "### Define the estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "rlTMZcywa0uo",
    "outputId": "3d8137ea-2f9c-4765-d179-53f06790874f"
   },
   "outputs": [],
   "source": [
    "# Define a PyTorch estimator\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# specify an output path\n",
    "# prefix is specified above\n",
    "output_path = 's3://{}/{}'.format(bucket, prefix)\n",
    "\n",
    "# instantiate  the estimator\n",
    "estimator = PyTorch(entry_point='train.py',\n",
    "                    source_dir='code', # train.py at code directory\n",
    "                    role=role,\n",
    "                    py_version='py38',\n",
    "                    framework_version='1.10.0', # PyTorch version\n",
    "                    instance_count=1,\n",
    "                    instance_type='ml.g4dn.xlarge',\n",
    "                    output_path=output_path,\n",
    "                    sagemaker_session=sagemaker_session,\n",
    "                    hyperparameters={\n",
    "                        'epochs': 50,\n",
    "                        'batch-size': 64,\n",
    "                        'lr': 0.001\n",
    "                    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h8_niqpia0up"
   },
   "source": [
    "### Train the estimator\n",
    "Train your estimator on the training data stored in S3. This should create a training job that you can monitor in your SageMaker console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "AsxfLM7Sa0ur"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-ap-south-1-461678052840/dog-breed-classifier/train\n",
      "s3://sagemaker-ap-south-1-461678052840/dog-breed-classifier/valid\n",
      "s3://sagemaker-ap-south-1-461678052840/dog-breed-classifier/test\n",
      "2022-05-20 07:12:53 Starting - Starting the training job...\n",
      "2022-05-20 07:13:20 Starting - Preparing the instances for trainingProfilerReport-1653030772: InProgress\n",
      "......\n",
      "2022-05-20 07:14:20 Downloading - Downloading input data.........\n",
      "2022-05-20 07:15:41 Training - Downloading the training image..................\n",
      "2022-05-20 07:18:55 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-05-20 07:18:57,591 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-05-20 07:18:57,614 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-05-20 07:18:57,622 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-05-20 07:18:58,002 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (4.61.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (1.10.0+cu113)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torchvision>=0.11.0 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (0.11.1+cu113)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch>=1.10.0->-r requirements.txt (line 2)) (3.10.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from torchvision>=0.11.0->-r requirements.txt (line 3)) (1.21.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow!=8.3.0,>=5.3.0 in /opt/conda/lib/python3.8/site-packages (from torchvision>=0.11.0->-r requirements.txt (line 3)) (8.3.2)\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2022-05-20 07:19:00,087 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\",\n",
      "        \"valid\": \"/opt/ml/input/data/valid\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 64,\n",
      "        \"epochs\": 50,\n",
      "        \"lr\": 0.001\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"valid\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2022-05-20-07-12-52-896\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-ap-south-1-461678052840/pytorch-training-2022-05-20-07-12-52-896/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch-size\":64,\"epochs\":50,\"lr\":0.001}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"valid\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\",\"valid\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-ap-south-1-461678052840/pytorch-training-2022-05-20-07-12-52-896/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\",\"valid\":\"/opt/ml/input/data/valid\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":64,\"epochs\":50,\"lr\":0.001},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"valid\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2022-05-20-07-12-52-896\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ap-south-1-461678052840/pytorch-training-2022-05-20-07-12-52-896/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch-size\",\"64\",\"--epochs\",\"50\",\"--lr\",\"0.001\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALID=/opt/ml/input/data/valid\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH-SIZE=64\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=50\u001b[0m\n",
      "\u001b[34mSM_HP_LR=0.001\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 train.py --batch-size 64 --epochs 50 --lr 0.001\u001b[0m\n",
      "\u001b[34mUsing GPU.\u001b[0m\n",
      "\u001b[34mDownloading: \"https://download.pytorch.org/models/resnet152-394f9c45.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-394f9c45.pth\u001b[0m\n",
      "\u001b[34m0%|          | 0.00/230M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 17.3M/230M [00:00<00:01, 181MB/s]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 34.7M/230M [00:00<00:01, 182MB/s]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 56.7M/230M [00:00<00:00, 204MB/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 78.2M/230M [00:00<00:00, 213MB/s]\u001b[0m\n",
      "\u001b[34m44%|████▎     | 101M/230M [00:00<00:00, 221MB/s]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 128M/230M [00:00<00:00, 242MB/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 154M/230M [00:00<00:00, 254MB/s]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 182M/230M [00:00<00:00, 266MB/s]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 210M/230M [00:00<00:00, 275MB/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 230M/230M [00:00<00:00, 249MB/s]\u001b[0m\n",
      "\u001b[34mResNet-152 last fc layer: Linear(in_features=2048, out_features=1000, bias=True)\u001b[0m\n",
      "\u001b[34mOur fc layer: Linear(in_features=2048, out_features=133, bias=True)\u001b[0m\n",
      "\u001b[34m[2022-05-20 07:19:10.661 algo-1:32 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-05-20 07:19:10.768 algo-1:32 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2022-05-20 07:19:10.768 algo-1:32 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2022-05-20 07:19:10.769 algo-1:32 INFO hook.py:200] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2022-05-20 07:19:10.769 algo-1:32 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2022-05-20 07:19:10.769 algo-1:32 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2022-05-20 07:19:11.846 algo-1:32 INFO hook.py:591] name:fc.weight count_params:272384\u001b[0m\n",
      "\u001b[34m[2022-05-20 07:19:11.846 algo-1:32 INFO hook.py:591] name:fc.bias count_params:133\u001b[0m\n",
      "\u001b[34m[2022-05-20 07:19:11.847 algo-1:32 INFO hook.py:593] Total Trainable Params: 272517\u001b[0m\n",
      "\u001b[34m[2022-05-20 07:19:11.847 algo-1:32 INFO hook.py:424] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2022-05-20 07:19:11.848 algo-1:32 INFO hook.py:488] Hook is writing from the hook with pid: 32\u001b[0m\n",
      "\u001b[34mEpoch 1: #011Training Loss: 2.890 #011Validation Loss: 1.044\u001b[0m\n",
      "\u001b[34mSaving the model...\u001b[0m\n",
      "\u001b[34mEpoch 2: #011Training Loss: 1.380 #011Validation Loss: 0.644\u001b[0m\n",
      "\u001b[34mSaving the model...\u001b[0m\n",
      "\u001b[34mEpoch 3: #011Training Loss: 1.111 #011Validation Loss: 0.521\u001b[0m\n",
      "\u001b[34mSaving the model...\u001b[0m\n",
      "\u001b[34mEpoch 4: #011Training Loss: 1.022 #011Validation Loss: 0.468\u001b[0m\n",
      "\u001b[34mSaving the model...\u001b[0m\n",
      "\u001b[34mEpoch 5: #011Training Loss: 0.972 #011Validation Loss: 0.427\u001b[0m\n",
      "\u001b[34mSaving the model...\u001b[0m\n",
      "\u001b[34mEpoch 6: #011Training Loss: 0.929 #011Validation Loss: 0.423\u001b[0m\n",
      "\u001b[34mSaving the model...\u001b[0m\n",
      "\u001b[34mEpoch 7: #011Training Loss: 0.914 #011Validation Loss: 0.466\u001b[0m\n",
      "\u001b[34mEpoch 8: #011Training Loss: 0.872 #011Validation Loss: 0.449\u001b[0m\n",
      "\u001b[34mEpoch 9: #011Training Loss: 0.850 #011Validation Loss: 0.363\u001b[0m\n",
      "\u001b[34mSaving the model...\u001b[0m\n",
      "\u001b[34mEpoch 10: #011Training Loss: 0.829 #011Validation Loss: 0.389\u001b[0m\n",
      "\u001b[34mEpoch 11: #011Training Loss: 0.778 #011Validation Loss: 0.365\u001b[0m\n",
      "\u001b[34mEpoch 12: #011Training Loss: 0.789 #011Validation Loss: 0.373\u001b[0m\n",
      "\u001b[34mEpoch 13: #011Training Loss: 0.770 #011Validation Loss: 0.383\u001b[0m\n",
      "\u001b[34mEpoch 14: #011Training Loss: 0.756 #011Validation Loss: 0.368\u001b[0m\n",
      "\u001b[34mEpoch 15: #011Training Loss: 0.792 #011Validation Loss: 0.380\u001b[0m\n",
      "\u001b[34mEpoch 16: #011Training Loss: 0.762 #011Validation Loss: 0.371\u001b[0m\n",
      "\u001b[34mEpoch 17: #011Training Loss: 0.771 #011Validation Loss: 0.410\u001b[0m\n",
      "\u001b[34mEpoch 18: #011Training Loss: 0.744 #011Validation Loss: 0.375\u001b[0m\n",
      "\u001b[34mEpoch 19: #011Training Loss: 0.756 #011Validation Loss: 0.390\u001b[0m\n",
      "\u001b[34mEpoch 20: #011Training Loss: 0.766 #011Validation Loss: 0.335\u001b[0m\n",
      "\u001b[34mSaving the model...\u001b[0m\n",
      "\u001b[34mEpoch 21: #011Training Loss: 0.743 #011Validation Loss: 0.354\u001b[0m\n",
      "\u001b[34mEpoch 22: #011Training Loss: 0.693 #011Validation Loss: 0.391\u001b[0m\n",
      "\u001b[34mEpoch 23: #011Training Loss: 0.716 #011Validation Loss: 0.379\u001b[0m\n",
      "\u001b[34mEpoch 24: #011Training Loss: 0.709 #011Validation Loss: 0.375\u001b[0m\n",
      "\u001b[34mEpoch 25: #011Training Loss: 0.717 #011Validation Loss: 0.388\u001b[0m\n",
      "\u001b[34mEpoch 26: #011Training Loss: 0.717 #011Validation Loss: 0.353\u001b[0m\n",
      "\u001b[34mEpoch 27: #011Training Loss: 0.695 #011Validation Loss: 0.402\u001b[0m\n",
      "\u001b[34mEpoch 28: #011Training Loss: 0.701 #011Validation Loss: 0.363\u001b[0m\n",
      "\u001b[34mEpoch 29: #011Training Loss: 0.688 #011Validation Loss: 0.400\u001b[0m\n",
      "\u001b[34mEpoch 30: #011Training Loss: 0.689 #011Validation Loss: 0.381\u001b[0m\n",
      "\u001b[34mEpoch 31: #011Training Loss: 0.700 #011Validation Loss: 0.326\u001b[0m\n",
      "\u001b[34mSaving the model...\u001b[0m\n",
      "\u001b[34mEpoch 32: #011Training Loss: 0.712 #011Validation Loss: 0.394\u001b[0m\n",
      "\u001b[34mEpoch 33: #011Training Loss: 0.679 #011Validation Loss: 0.347\u001b[0m\n",
      "\u001b[34mEpoch 34: #011Training Loss: 0.704 #011Validation Loss: 0.390\u001b[0m\n",
      "\u001b[34mEpoch 35: #011Training Loss: 0.691 #011Validation Loss: 0.323\u001b[0m\n",
      "\u001b[34mSaving the model...\u001b[0m\n",
      "\u001b[34mEpoch 36: #011Training Loss: 0.692 #011Validation Loss: 0.361\u001b[0m\n",
      "\u001b[34mEpoch 37: #011Training Loss: 0.704 #011Validation Loss: 0.403\u001b[0m\n",
      "\u001b[34mEpoch 38: #011Training Loss: 0.667 #011Validation Loss: 0.361\u001b[0m\n",
      "\u001b[34mEpoch 39: #011Training Loss: 0.686 #011Validation Loss: 0.347\u001b[0m\n",
      "\u001b[34mEpoch 40: #011Training Loss: 0.653 #011Validation Loss: 0.365\u001b[0m\n",
      "\u001b[34mEpoch 41: #011Training Loss: 0.641 #011Validation Loss: 0.353\u001b[0m\n",
      "\u001b[34mEpoch 42: #011Training Loss: 0.647 #011Validation Loss: 0.386\u001b[0m\n",
      "\u001b[34mEpoch 43: #011Training Loss: 0.676 #011Validation Loss: 0.381\u001b[0m\n",
      "\u001b[34mEpoch 44: #011Training Loss: 0.641 #011Validation Loss: 0.381\u001b[0m\n",
      "\u001b[34mEpoch 45: #011Training Loss: 0.673 #011Validation Loss: 0.372\u001b[0m\n",
      "\u001b[34mEpoch 46: #011Training Loss: 0.659 #011Validation Loss: 0.342\u001b[0m\n",
      "\u001b[34mEpoch 47: #011Training Loss: 0.642 #011Validation Loss: 0.416\u001b[0m\n",
      "\u001b[34mEpoch 48: #011Training Loss: 0.669 #011Validation Loss: 0.397\u001b[0m\n",
      "\u001b[34mEpoch 49: #011Training Loss: 0.662 #011Validation Loss: 0.446\u001b[0m\n",
      "\u001b[34mEpoch 50: #011Training Loss: 0.700 #011Validation Loss: 0.374\u001b[0m\n",
      "\u001b[34m2022-05-20 08:44:45,760 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2022-05-20 08:45:00 Uploading - Uploading generated training model\n",
      "2022-05-20 08:45:40 Completed - Training job completed\n",
      "ProfilerReport-1653030772: IssuesFound\n",
      "Training seconds: 5470\n",
      "Billable seconds: 5470\n",
      "CPU times: user 11.6 s, sys: 445 ms, total: 12 s\n",
      "Wall time: 1h 33min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "\n",
    "train_path = os.path.join(input_dataset, 'train')\n",
    "valid_path = os.path.join(input_dataset, 'valid')\n",
    "test_path = os.path.join(input_dataset, 'test')\n",
    "print(train_path)\n",
    "print(valid_path)\n",
    "print(test_path)\n",
    "\n",
    "# Train your estimator on S3 training data\n",
    "estimator.fit({ 'train': train_path,\n",
    "                'valid': valid_path,\n",
    "                'test': test_path\n",
    "              })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the estimator\n",
    "\n",
    "After training, deploy your model to create a predictor. If you're using a PyTorch model, you'll need to create a trained PyTorchModel that accepts the trained <model>.model_data as an input parameter and points to the provided source_pytorch/predict.py file as an entry point.\n",
    "\n",
    "To deploy a trained model, you'll use `model.deploy`, which takes in two arguments:\n",
    "\n",
    "- initial_instance_count: The number of deployed instances (1).\n",
    "- instance_type: The type of SageMaker instance for deployment.\n",
    ">Note: If you run into an instance error, it may be because you chose the wrong training or deployment instance_type. It may help to refer to your previous exercise code to see which types of instances we used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "\n",
    "# deploy your model to create a predictor\n",
    "predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.t2.medium')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "name": "dog_app.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
  },
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
